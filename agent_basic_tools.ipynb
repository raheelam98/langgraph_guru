{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/langgraph_guru/blob/main/agent_basic_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "sYy5VpP6O9Db"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith langchain_google_genai langchain   python-dotenv"
      ],
      "metadata": {
        "id": "7rEZvyK3_SRd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API keys set up\n",
        "# Configure environment to connect to LangSmith.\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-basic-tool\"\n",
        "\n",
        "# LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n"
      ],
      "metadata": {
        "id": "J3B8gzBM_TSv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "9vJyMVlH_d42"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "mgk9OiQg_jcN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p51IQ8sXP636"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"hi\")"
      ],
      "metadata": {
        "id": "ZYkq5BwMQRSk",
        "outputId": "67df579d-f50d-4645-b8e0-2ee7c0c6803f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-087311a4-9031-4c85-ae44-0685f14b683a-0', usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"hi\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "W-QTo2vXPsRw",
        "outputId": "5a9a8e8f-b1be-48c1-e7b1-d55eeba24c74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi there! How can I help you today?\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-4a15225d-b000-4cd8-a44c-b5610549209d-0' usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# packages\n",
        "%%capture --no-stderr\n",
        "%pip install -U tavily-python langchain_community"
      ],
      "metadata": {
        "id": "Bqjh0xQR_oAq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API keys set up\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "id": "iysV3HFT_pch"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lang Chain and Lang Graph Documents**"
      ],
      "metadata": {
        "id": "IcUex0wB-_se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent Basic**\n",
        "\n",
        "* ToolNode\n",
        "* ToolMessage\n"
      ],
      "metadata": {
        "id": "r0jXCpr0Gu5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)\n",
        "\n",
        "**JsonOutputParser :** Parse the output of an LLM call to a JSON object.\n",
        "\n",
        "When used in streaming mode, it will yield partial JSON objects containing all the keys that have been returned so far.\n",
        "\n",
        "In streaming, if diff is set to True, yields JSONPatch operations describing the difference between the previous and the current object."
      ],
      "metadata": {
        "id": "ZF4SIQMr_4sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ToolExecutorAgent (Session 3 - Lang Graph)\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    missing: str = Field(description=\"Critique of what is missing.\")\n",
        "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
        "\n",
        "\n",
        "class AnswerQuestion(BaseModel):\n",
        "    \"\"\"Answer the question.\"\"\"\n",
        "\n",
        "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
        "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
        "    search_queries: List[str] = Field(\n",
        "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
        "    )\n",
        "\n",
        "# ToolMessage\n",
        "from ast import parse\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage, BaseMessage, AIMessage\n",
        "from typing import List\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "parser = JsonOutputParser(return_id=True)\n",
        "\n",
        "def exeute_tools(state: List[BaseMessage])-> List[ToolMessage]:\n",
        "  tool_invocation : AIMessage = state[-1]\n",
        "  parsed_tool_calls = parser.invoke(tool_invocation)\n",
        "\n",
        "  ids = []\n",
        "  tool_invocation = []\n",
        "\n",
        "  for parsed_call in parsed_tool_calls:\n",
        "    for query in parsed_call[\"args\"][\"search_queries\"]:\n",
        "      print(query)\n",
        "      tool_invocation.append(ToolInvocation(\n",
        "        tool=\"tavily_search_results_json\",\n",
        "        tool_input=query\n",
        "      ))\n",
        "      ids.append(parsed_call[\"id\"])\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "Un0njCZ9GuLX"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}