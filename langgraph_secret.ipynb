{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+rRkL8GzyAGO5Xfg0moxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/langgraph_guru/blob/main/langgraph_secret.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langgraph Secret**"
      ],
      "metadata": {
        "id": "RQMXQYDdDz_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low_level/)\n",
        "\n",
        "**`StateGraph class`** is the main graph class\n",
        "\n",
        "**`MessageGraph class`**  is a special graph type where the state is only a list of messages, mainly used for chatbots as it simplifies state management.\n",
        "\n",
        "MessageGraph as a subclass of StateGraph\n",
        "\n",
        "\n",
        "Annotate the 'messages' key to use the 'add_messages' reducer function for appending messages"
      ],
      "metadata": {
        "id": "J8VZ_5DtEbq0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtdMMASRDYZ6"
      },
      "outputs": [],
      "source": [
        "class MessageGraph(StateGraph):\n",
        "  \"\"\"A StateGraph where every node\n",
        "  - receives a list of message\n",
        "  - returns one or more messages ouput\n",
        "   \"\"\"\n",
        "\n",
        "   def __init__(self) -> None:\n",
        "      super().__init__Annotated[list[AnyMessage], add_messages]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OPENAI_API_KEY=sk- your key\n",
        "* GEMINI_API_KEY= your key\n",
        "* LANGCHAIN_API_KEY=ls your key\n",
        "* LANGCHAIN_TRACING_V2=true\n",
        "* LANGCHAIN_PROJECT=project name\n",
        "\n",
        "Use either **`OPENAI_API_KEY`** or **`GEMINI_API_KEY`**"
      ],
      "metadata": {
        "id": "ZupcsNZkrI1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Converstion Flow**"
      ],
      "metadata": {
        "id": "_ZOTr2HCg8zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt** to predict, review the output, revise it, and provide suggestions for improvement"
      ],
      "metadata": {
        "id": "VdsClFlsprQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the reflector chain and the tweet reviosr chain\n",
        "# Prompt to predict, review the output, revise it, and provide suggestions for improvement\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "#from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#Load environment variables from .env file\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "KH4Dei-Apfer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt to predict, review the output, revise it, and provide suggestions for improvement\n",
        "reflection_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a viral twitter influencer grading a tweet. Generate critique and recommendation for the user.\"\n",
        "            \"Always provide detailed recommendations, including requests for length, virality, style, etc.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lj4vK44Fg6-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a message and continuously revise it based on feedback\n",
        "# from the reflection prompt until achieve the perfect result.\n",
        "generation_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "           \"system\",\n",
        "           \"You are a twitter techie influencer assistant tasked with writing excellent twitter posts.\"\n",
        "           \"Generate the best twitter post possible for the user's request.\"\n",
        "           \"If the user provides critique, respond with a revised version of your previous attempts.\"\n",
        "        ),\n",
        "        # placehoder for reflection and revision\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0rxP4GpPhGCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect Prompt to LLM and Create Chain**"
      ],
      "metadata": {
        "id": "lXx15LIbiIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Prompt to LLM and Create Chain\n",
        "\n",
        "# Initialize the language model as an instance of ChatOpenAI.\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "# Create the generation chain by combining the generation prompt with the language model.\n",
        "generation_chain = generation_prompt | llm\n",
        "\n",
        "# Create the reflection chain by combining the reflection prompt with the language model.\n",
        "reflection_chain = reflection_prompt | llm\n"
      ],
      "metadata": {
        "id": "hMLON9CVhQrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Graph for Converstion Flow**"
      ],
      "metadata": {
        "id": "InHiRP_7p8Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Sequence\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langgraph.graph import END, MessageGraph\n",
        "\n",
        "from chain import generation_chain, reflection_chain\n",
        "\n",
        "#Load environment variables from .env file\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "5b6J66iEp3xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Nodes**"
      ],
      "metadata": {
        "id": "_icivBFWi3SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define nodes name\n",
        "REFLECT = \"reflect\"\n",
        "GENERATE = \"generate\""
      ],
      "metadata": {
        "id": "hO8DnLKUi1tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Flow (Part 1)\n",
        "\n",
        "**Input Messages**\n",
        "\n",
        "The **generate node receives an input state**, where the state is a sequence of messages. It runs the generation chain and invokes all the states, essentially taking a tweet and revising it according to the state."
      ],
      "metadata": {
        "id": "unQS1xGGL40u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generation node function which receives an input\n",
        "# state (a sequence of messages).\n",
        "def generation_node(state: Sequence[BaseMessage]):\n",
        "    # Invoke the generation chain with the current state,\n",
        "    # where state is passed as a dictionary with \"messages\" as the key.\n",
        "    return generation_chain.invoke({\"messages\": state})\n"
      ],
      "metadata": {
        "id": "10pmhR4OMFAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Flow (Part 2)\n",
        "\n",
        "**Receive Messages**\n",
        "\n",
        "Once it **receives a sequence of messages and invokes the chain**, as in the generation node:\n",
        "\n",
        "Difference in this function:\n",
        "\n",
        "* The response comes back from the LLM (usually in the role of AI).\n",
        "\n",
        "* We transform this response into a human message.\n",
        "\n",
        "* Take the content from the message and treat it as if a human sent it.\n",
        "\n",
        "* Train the message with the role of a human and then return it.\n",
        "\n",
        "**Reason :** By treating the LLM as if it were a human sending these messages, we maintain the conversation flow."
      ],
      "metadata": {
        "id": "lU-E4D1hUnlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the reflection node function which receives a sequence of messages.\n",
        "def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
        "    # Invoke the reflection chain with the current messages,\n",
        "    # where messages are passed as a dictionary with \"messages\" as the key.\n",
        "    res = reflection_chain.invoke({\"messages\": messages})\n",
        "\n",
        "    # Return the response as a list containing HumanMessage,\n",
        "    # transforming the content from the LLM's response into a human message.\n",
        "    return [HumanMessage(content=res.content)]\n"
      ],
      "metadata": {
        "id": "clnkMnU9Uody"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Graph to Process up to 6 Messages**"
      ],
      "metadata": {
        "id": "wHBtMrVJq0_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MessageGraph builder\n",
        "builder: MessageGraph = MessageGraph()\n",
        "\n",
        "# Add the generate node to the graph\n",
        "builder.add_node(GENERATE, generation_node)\n",
        "\n",
        "# Add the reflect node to the graph\n",
        "builder.add_node(REFLECT, reflection_node)\n",
        "\n",
        "# Set the entry point of the graph to the generate node (commented out alternative version)\n",
        "# builder.set_entry_point(generation_node)\n",
        "\n",
        "# Set the entry point of the graph to the generate node using the node name\n",
        "builder.set_entry_point(GENERATE)\n",
        "\n",
        "# Define a function to determine if the process should continue\n",
        "def should_continue(state: List[BaseMessage]):\n",
        "    if len(state) > 6:\n",
        "        return END\n",
        "    return REFLECT\n",
        "\n",
        "# Add conditional edges from the generate node based on the should_continue function\n",
        "builder.add_conditional_edges(GENERATE, should_continue)\n",
        "\n",
        "# Add an edge from the reflect node back to the generate node\n",
        "builder.add_edge(REFLECT, GENERATE)\n",
        "\n",
        "# Compile the graph\n",
        "graph = builder.compile()\n",
        "\n",
        "# Print the graph representation in Mermaid syntax\n",
        "print(graph.get_graph().draw_mermaid())\n",
        "\n",
        "# Main entry point\n",
        "if __name__ == '__main__':\n",
        "    print(\"hi LangGraph\")\n"
      ],
      "metadata": {
        "id": "t07JlCbnqLZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}