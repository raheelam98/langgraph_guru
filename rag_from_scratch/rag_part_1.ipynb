{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/langgraph_guru/blob/main/rag_from_scratch/rag_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leaderboard\n",
        "\n",
        "1.   [Arena](https://lmarena.ai/)\n",
        "2.   [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)\n",
        "3. sora video\n",
        "\n",
        "4.   [huggingface](https://huggingface.co/spaces/ArtificialAnalysis/Video-Generation-Arena-Leaderboard)\n",
        "\n"
      ],
      "metadata": {
        "id": "MdDpwM3m76S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG From Scratch\n",
        "\n",
        "**Tutorials**\n",
        "\n",
        "[Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
        "\n",
        "[Build a Retrieval Augmented Generation (RAG) App: Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history/)\n",
        "\n",
        "\n",
        "[Making it easier to build human-in-the-loop agents with interrupt](https://blog.langchain.dev/making-it-easier-to-build-human-in-the-loop-agents-with-interrupt/)"
      ],
      "metadata": {
        "id": "kpyoNgRtFeiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG From Scratch\n",
        "\n",
        "[Build a Retrieval Augmented Generation (RAG) App: Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
        "\n",
        "\n",
        "[RAG From Scratch - videos](https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\n",
        "\n",
        "[rag-from-scratch - github\n",
        "](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb)\n",
        "\n",
        "**Basic**\n",
        "\n",
        "* Indexting\n",
        "* Retrieval\n",
        "* Generation\n",
        "\n",
        "**Advance**\n",
        "\n",
        "* Query transformation\n",
        "* Routing\n",
        "* Query contruction\n",
        "* Indexing\n",
        "* Retrieval\n",
        "* Grenration\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w3ZfY_MXHQsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Generative AI Embeddings**"
      ],
      "metadata": {
        "id": "5PP-SJKHFnvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TuZLndVL4E6w"
      },
      "outputs": [],
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install --upgrade --quiet  langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages:\n",
        "%%capture --no-stderr\n",
        "%pip install langchain_community tiktoken langchainhub chromadb langchain\n"
      ],
      "metadata": {
        "id": "2DAOi_HTK25R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag_ai_agent\""
      ],
      "metadata": {
        "id": "y7IC3KGJHOfE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys\n",
        "# Get the GEMINI API key from user data\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "vwdw9YxgHZ7T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGoogleGenerativeAI with the Gemini model\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key    # Provide the Google API key for authentication\n",
        ")"
      ],
      "metadata": {
        "id": "xlR8Y8TBHm7M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LLM with a query\n",
        "result = llm.invoke(\"hi\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L02kbDtVHo0D",
        "outputId": "376817ae-41b9-4ff1-8a58-c51df16d9832"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-d9752be9-fa39-4223-a18d-aecf440e35e2-0', usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"Hi\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kG8n0TSEamR",
        "outputId": "46540921-0289-498e-9d71-2f9e7939e8df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi there! How can I help you today?\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-bc402e3e-db64-444b-a0de-c3a2b02067a6-0' usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(\n",
        "#     model=\"models/embedding-001\" , # Specify the embedding model to use\n",
        "#     api_key=gemini_api_key\n",
        "# )#vector = embeddings.embed_query(\"hello, world!\")\n",
        "# # vector[:5]\n",
        "\n",
        "# embeddings.embed_query(\"Hello\")\n"
      ],
      "metadata": {
        "id": "Tmm6FN-hH8KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xycVarUVJl69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embedding**\n",
        "\n",
        "[Vector embeddings](https://platform.openai.com/docs/guides/embeddings#what-are-embeddings)\n",
        "\n",
        "```\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\" ,\n",
        "    google_api_key=gemini_api_key  # correct\n",
        "    )\n",
        "\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]    \n",
        "```"
      ],
      "metadata": {
        "id": "lfcD47-5IgyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" , google_api_key=gemini_api_key)\n",
        "\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:4]\n",
        "\n",
        "# embeddings.embed_query(\"Hello\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q08H204aR9Qg",
        "outputId": "bec31843-9fae-4788-ec59-25effd34613d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734263241291]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\" ,\n",
        "    google_api_key=gemini_api_key  # correct\n",
        "    )\n",
        "\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l8ySXxTVoBi",
        "outputId": "f007c432-4a7a-4312-b019-19466c1f6359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734263241291,\n",
              " 0.01813093200325966,\n",
              " -0.0018945280462503433,\n",
              " 0.028477225452661514,\n",
              " -0.007562300190329552,\n",
              " 0.011064725928008556,\n",
              " -0.005353901535272598]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexting - Part 2"
      ],
      "metadata": {
        "id": "4lLqLibzJH9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\""
      ],
      "metadata": {
        "id": "gSiaiZ6BKOCR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many tokens a string has before I embed it**\n",
        "\n",
        "**Count tokens with Tiktoken**\n",
        "\n",
        "[How to count tokens with tiktoken\n",
        "](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "KtkjMj4PP-KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many tokens a string has before I embed it\n",
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrXE4GhSKjc2",
        "outputId": "55f41dd6-a060-42dd-87d5-3b63d53e2fec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text embedding models**\n",
        "\n",
        "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai/)"
      ],
      "metadata": {
        "id": "FP4glzeJUf76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\" ,\n",
        "    google_api_key=gemini_api_key  # correct\n",
        "    )\n",
        "\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "\n",
        "len(query_result)\n",
        "\n",
        "#print(type(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaeiOOF9L_4O",
        "outputId": "841efb93-f3a7-45a7-afef-832a73e4a516"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(query_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxuluDSvKjNn",
        "outputId": "f99702c7-95cd-4c99-ff1b-f01ed5edc5ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity is reccomended (1 indicates identical) for OpenAI embeddings.**\n",
        "\n",
        "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings#faq)"
      ],
      "metadata": {
        "id": "Fy1uM_lrVcaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY461QSbVjWL",
        "outputId": "baf64edf-fe40-4eca-ae0e-0e279eacd6af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.8535652119095083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Loaders**\n",
        "\n",
        "[Document loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
      ],
      "metadata": {
        "id": "kpcpjz_2XwWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set a custom User-Agent string\n",
        "os.environ[\"USER_AGENT\"] = \"WebBaseLoaderBot/1.0 (https://lilianweng.github.io/posts/2023-06-23-agent/)\""
      ],
      "metadata": {
        "id": "DQwmq0O0eeNe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "id": "Hy1CLgfZX83R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[How to recursively split text by characters](https://python.langchain.com/docs/how_to/recursive_text_splitter/)"
      ],
      "metadata": {
        "id": "8efSd3bPAQSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "id": "bG-vrd13fPre"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Vector stores](https://python.langchain.com/docs/integrations/vectorstores/)"
      ],
      "metadata": {
        "id": "IoO2Z9N3VkgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Index\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\" ,\n",
        "    google_api_key=gemini_api_key  # correct\n",
        "    )\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embd)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "GvYevHZNfUaU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Retrieval"
      ],
      "metadata": {
        "id": "smqJZwpYWZQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embd)\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "5v7PXGBMSRlD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5JPQBxiAiZC",
        "outputId": "146e5200-fa9b-4b8b-cc1a-07da39aa512b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-199d1ce08656>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atokYrBKAjev",
        "outputId": "ab8f8e2f-cc34-4888-ae3d-bc07c7b1be13"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Generation"
      ],
      "metadata": {
        "id": "I5Y6afOrCcGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGN3wL7bC0ji",
        "outputId": "836658c8-02aa-43fd-cf8f-510dc337a11c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "# from google.colab import userdata\n",
        "# gemini_api_key = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=gemini_api_key)\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "_KaSO50xDDaP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "3kastCegFS28"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run  (ERROR)\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
      ],
      "metadata": {
        "id": "njYT_BxCFW-d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "k2rclm0QFf5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "id": "AZoAwwO8Fzg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG chains**"
      ],
      "metadata": {
        "id": "83U703GjT79y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "IF1js29EUAd9",
        "outputId": "314483d3-4e11-46db-b19d-2e48a6f01a72"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided text, task decomposition is the process of breaking down a complex task into smaller, simpler steps.  This can be achieved through prompting an LLM (large language model) with instructions like \"Steps for XYZ...,\"  using task-specific instructions (e.g., \"Write a story outline\"), or with human input.  Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this process by guiding the model\\'s reasoning step-by-step or exploring multiple reasoning possibilities, respectively.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to Debug**"
      ],
      "metadata": {
        "id": "qjcUTQKvJs2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 -  Check gemini_api_key Initialization:\n",
        "\n",
        "# print(f\"gemini_api_key: {gemini_api_key}\")\n"
      ],
      "metadata": {
        "id": "HGywsWJ8JsLW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 -  Validate Splits:\n",
        "\n",
        "print(f\"Number of splits: {len(splits)}\")\n",
        "print(splits[0] if splits else \"No splits found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh6WbxJFK-dR",
        "outputId": "37ed213f-2e35-4966-dfa1-fc52d21c16ae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits: 52\n",
            "page_content='LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 - Check Embeddings Initialization:\n",
        "\n",
        "print(f\"Embedding Model: {embd}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ihcus-SLVvL",
        "outputId": "e32789fe-a49b-4388-9007-19508a72c1d3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model: client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ffaec5f1f00> model='models/embedding-001' task_type=None google_api_key=SecretStr('**********') credentials=None client_options=None transport=None request_options=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 - Ensure vectorstore Setup:\n",
        "\n",
        "print(f\"VectorStore: {vectorstore}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_trn41bgLkmg",
        "outputId": "caa42f60-55cd-4808-b457-42b8840b656c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorStore: <langchain_community.vectorstores.chroma.Chroma object at 0x7ffaecc9c370>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 - Validate Retrieved Documents:\n",
        "\n",
        "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "print(f\"Number of retrieved docs: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LpVnKVVL4sz",
        "outputId": "61891605-9fcc-48a8-bac8-a70ec6a34806"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of retrieved docs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test code**"
      ],
      "metadata": {
        "id": "kjbM34PxMbnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure gemini_api_key is defined\n",
        "assert gemini_api_key, \"API Key is not defined!\"\n",
        "\n",
        "# Check if blog_docs and splits are populated\n",
        "assert blog_docs, \"Failed to load blog documents!\"\n",
        "assert splits, \"Document splitting failed!\"\n",
        "\n",
        "# Validate embedding initialization\n",
        "try:\n",
        "    embd = GoogleGenerativeAIEmbeddings(\n",
        "        model=\"models/embedding-001\",\n",
        "        google_api_key=gemini_api_key\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error initializing embeddings: {e}\")\n",
        "\n",
        "print(f\"Embedding Model: {embd}\")\n",
        "\n",
        "# Validate vector store\n",
        "try:\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embd)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating vectorstore: {e}\")\n",
        "\n",
        "print(f\"\\nVectorStore: {vectorstore}\")\n",
        "\n",
        "# Check retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "print(f\"\\nretriever: {retriever}\")\n",
        "\n",
        "# Retrieve documents\n",
        "try:\n",
        "    docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "    assert docs, \"No relevant documents found!\"\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error retrieving documents: {e}\")\n",
        "\n",
        "print(f\"\\nNumber of retrieved docs: {len(docs)}\")\n",
        "\n",
        "\n",
        "# # Chain setup and invocation\n",
        "# try:\n",
        "#     response = chain.invoke({\"context\": docs, \"question\": \"What is Task Decomposition?\"})\n",
        "#     print(response)\n",
        "# except Exception as e:\n",
        "#     raise ValueError(f\"Error running chain: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cqSuDSYMexT",
        "outputId": "cf60c652-ca39-4a4c-97ff-da2ae18f5507"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model: client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ffaec96fd00> model='models/embedding-001' task_type=None google_api_key=SecretStr('**********') credentials=None client_options=None transport=None request_options=None\n",
            "\n",
            "VectorStore: <langchain_community.vectorstores.chroma.Chroma object at 0x7ffaec96c3a0>\n",
            "\n",
            "retriever: tags=['Chroma', 'GoogleGenerativeAIEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7ffaec96c3a0> search_kwargs={'k': 1}\n",
            "\n",
            "Number of retrieved docs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Chain setup and invocation  (ERROR)\n",
        "# try:\n",
        "#     response = chain.invoke({\"context\": docs, \"question\": \"What is Task Decomposition?\"})\n",
        "#     print(response)\n",
        "# except Exception as e:\n",
        "#     raise ValueError(f\"Error running chain: {e}\")\n",
        "\n",
        "# print(f\"response: {response}\")\n"
      ],
      "metadata": {
        "id": "3a1UO1AVOsbW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test 2**"
      ],
      "metadata": {
        "id": "86kKYh6KQdjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure the API Key Is Correct and Passed Properly:\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=gemini_api_key\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=gemini_api_key\n",
        ")\n"
      ],
      "metadata": {
        "id": "0sKXu4gVQdPY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test API Key Validity: (ERROR)\n",
        "\n",
        "# from google.auth.transport.requests import Request\n",
        "# from google.oauth2.credentials import Credentials\n",
        "\n",
        "# # Test the key with a simple request\n",
        "# creds = Credentials(token=gemini_api_key)\n",
        "# request = Request()\n",
        "# try:\n",
        "#     creds.refresh(request)\n",
        "#     print(\"API Key is valid.\")\n",
        "# except Exception as e:\n",
        "#     raise ValueError(f\"Invalid API Key: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dug9IXNAQ9FY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Embedding Model: {embd}\")\n",
        "print(f\"\\nLLM Model: {llm}\")\n",
        "print(f\"\\nRetrieved Docs: {docs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWNrxbghS10W",
        "outputId": "f4a82633-619b-4c8d-9f1a-6aced94cfa19"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Model: client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ffaec96fd00> model='models/embedding-001' task_type=None google_api_key=SecretStr('**********') credentials=None client_options=None transport=None request_options=None\n",
            "\n",
            "LLM Model: model='models/gemini-1.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ffaec9b5570> default_metadata=()\n",
            "\n",
            "Retrieved Docs: [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare context as plain text.     # ERROR\n",
        "# context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# # Chain setup and invocation\n",
        "# try:\n",
        "#     response = chain.invoke({\"context\": context, \"question\": \"What is Task Decomposition?\"})\n",
        "#     print(f\"response: {response}\")\n",
        "# except Exception as e:\n",
        "#     raise ValueError(f\"Error running chain: {e}\")\n"
      ],
      "metadata": {
        "id": "Vj5fi5QOTSTr"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Context: {context}\")  # ERROR"
      ],
      "metadata": {
        "id": "qdTB206eTDJt"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}